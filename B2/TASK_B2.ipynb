{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9c5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa55715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess training set\n",
    "#splits eye_color and image name out of the training data and forms independent dataframe out of them\n",
    "data=pd.read_csv('../Datasets/cartoon_set/labels.csv')\n",
    "\n",
    "labelsTrain = data[\"\\teye_color\\tface_shape\\tfile_name\"].str.split(pat=\"\\t\", n=-1, expand=True)\n",
    "labelsTrain.drop(columns =[0,2], inplace = True)\n",
    "labelsTrain.columns = [ \"eye_color\" , \"img_name\"]\n",
    "labelsTrain = labelsTrain.astype({'eye_color': 'int32'})\n",
    "labelsTrain = labelsTrain.astype({'img_name': 'string'})\n",
    "\n",
    "#Preprocess testing set\n",
    "#splits eye_color and image name out of the testing data and forms independent dataframe out of them\n",
    "data=pd.read_csv('../Datasets/cartoon_set_test/labels.csv')\n",
    "\n",
    "labelsTest = data[\"\\teye_color\\tface_shape\\tfile_name\"].str.split(pat=\"\\t\", n=-1, expand=True)\n",
    "labelsTest.drop(columns =[0,2], inplace = True)\n",
    "labelsTest.columns = [ \"eye_color\" , \"img_name\"]\n",
    "labelsTest = labelsTest.astype({'eye_color': 'int32'})\n",
    "labelsTest = labelsTest.astype({'img_name': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab8a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd9218e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778d082e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c7b1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training set################################\n",
    "#reads image data from training set ,resises it, flattens it , and stores it into imageTrain array \n",
    "imagesTrain = np.zeros((10000, 50 , 50, 3)) \n",
    "\n",
    "for i in range(0,10000):\n",
    "    image = cv.imread('../Datasets/cartoon_set/img/' + labelsTrain[\"img_name\"][i])\n",
    "    image = image[230:290,180:320] # cut 95 from left / right sides , 75 from top, 80 from the bottom\n",
    "    image = cv.resize(image, dsize=(50 , 50), interpolation=cv.INTER_CUBIC)\n",
    "    imagesTrain[i] = image\n",
    "    \n",
    "imagesTrain = imagesTrain.reshape(10000,7500)    \n",
    "imagesTrain = pd.DataFrame(imagesTrain) \n",
    "\n",
    "\n",
    "#Load Testing set################################\n",
    "#reads image data from Testing set ,resises it, flattens it , and stores it into imageTest array \n",
    "imagesTest = np.zeros((2500, 50 , 50, 3))\n",
    "\n",
    "for i in range(0,2500):\n",
    "    image = cv.imread('../Datasets/cartoon_set_test/img/' + labelsTest[\"img_name\"][i])\n",
    "    image = image[230:290,180:320]\n",
    "    image = cv.resize(image, dsize=(50 , 50), interpolation=cv.INTER_CUBIC)\n",
    "    imagesTest[i] = image\n",
    "    \n",
    "    \n",
    "imagesTest = imagesTest.reshape(2500,7500)   \n",
    "imagesTest = pd.DataFrame(imagesTest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95d126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "c0b0795a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b698b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ab81d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolves flattened data of imageTrain and ImageTest with 1D 3 size filter and then stores it into x_train and x_test array respectively\n",
    "x_train=np.zeros((10000,7502))\n",
    "for i in range(0,10000):\n",
    "    x_train[i]=np.convolve(imagesTrain.iloc[i,:],[0.066,-5.599,5.566])\n",
    "    \n",
    "x_test=np.zeros((2500,7502))\n",
    "for i in range(0,2500):\n",
    "    x_test[i]=np.convolve(imagesTest.iloc[i,:],[0.066,-5.599,5.566])\n",
    "\n",
    "x_train = pd.DataFrame(x_train) \n",
    "x_test = pd.DataFrame(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3dfcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7dcf310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8152\n",
      "0.8272\n",
      "0.8292\n",
      "0.8308\n",
      "0.8296\n",
      "0.8324\n",
      "0.8312\n",
      "0.8344\n",
      "0.8304\n"
     ]
    }
   ],
   "source": [
    "#runs knn algorithem for n_neighbor values between 100 and 1000 in steps of 100\n",
    "for i in range(100,1000,100):\n",
    "    model = KNeighborsClassifier(n_neighbors=i, weights = 'distance' , n_jobs=-1)\n",
    "    model.fit(x_train,labelsTrain['eye_color'])\n",
    "    print(model.score(x_test,labelsTest['eye_color']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adb188e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets hyper parameter testing for cross validation\n",
    "hyper_params = {\n",
    "    'weights' : ['uniform', 'distance'],\n",
    "    'n_neighbors' : [100, 300, 500, 800, 1000],\n",
    "    'p' : [1 , 2],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e4032c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines crossvalidation function\n",
    "model = KNeighborsClassifier(n_jobs = -1)\n",
    "grid=GridSearchCV(model , param_grid=hyper_params , cv=10 , n_jobs=-1 ,  verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8506ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=KNeighborsClassifier(n_jobs=-1), n_jobs=-1,\n",
       "             param_grid={&#x27;n_neighbors&#x27;: [100, 300, 500, 800, 1000], &#x27;p&#x27;: [1, 2],\n",
       "                         &#x27;weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]},\n",
       "             verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=KNeighborsClassifier(n_jobs=-1), n_jobs=-1,\n",
       "             param_grid={&#x27;n_neighbors&#x27;: [100, 300, 500, 800, 1000], &#x27;p&#x27;: [1, 2],\n",
       "                         &#x27;weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]},\n",
       "             verbose=10)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_jobs=-1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_jobs=-1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10, estimator=KNeighborsClassifier(n_jobs=-1), n_jobs=-1,\n",
       "             param_grid={'n_neighbors': [100, 300, 500, 800, 1000], 'p': [1, 2],\n",
       "                         'weights': ['uniform', 'distance']},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#starts cross validation\n",
    "grid.fit(x_train,labelsTrain['eye_color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de6ac26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_jobs=-1, n_neighbors=100, p=1, weights=&#x27;distance&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_jobs=-1, n_neighbors=100, p=1, weights=&#x27;distance&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_jobs=-1, n_neighbors=100, p=1, weights='distance')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross valdiation best estimator\n",
    "grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe83ac03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8322999999999998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross validation best score\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28090f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8236\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_jobs=-1, n_neighbors=100, p=1, weights='distance')\n",
    "model.fit(x_train,labelsTrain['eye_color'])\n",
    "print(model.score(x_test,labelsTest['eye_color']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addef219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4422912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf09d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8f9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8604f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of gpus available:  1\n"
     ]
    }
   ],
   "source": [
    "#required Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten\n",
    "print(\"num of gpus available: \",len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "from keras.utils import normalize,to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63580d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training set for CNN################################\n",
    "#reads image data from training set ,resises it\n",
    "imagesTrain = np.zeros((10000, 150 , 150, 3)) \n",
    "\n",
    "for i in range(0,10000):\n",
    "    image = cv.imread('C:/Users/X99S5/Documents/AMLS_22-23_SN18046828/Datasets/cartoon_set/img/' + labelsTrain[\"img_name\"][i])\n",
    "    image = image[75:420,95:405] # cut 95 from left / right sides , 75 from top, 80 from the bottom\n",
    "    image = cv.resize(image, dsize=(150 , 150), interpolation=cv.INTER_CUBIC)\n",
    "\n",
    "    imagesTrain[i] = image\n",
    "\n",
    "\n",
    "\n",
    "#Load Testing set for CNN################################\n",
    "#reads image data from testing set ,resises it\n",
    "imagesTest = np.zeros((2500, 150 , 150, 3))\n",
    "\n",
    "for i in range(0,2500):\n",
    "    image = cv.imread('C:/Users/X99S5/Documents/AMLS_22-23_SN18046828/Datasets/cartoon_set_test/img/' + labelsTest[\"img_name\"][i])\n",
    "    image = image[75:420,95:405]\n",
    "    image = cv.resize(image, dsize=(150 , 150), interpolation=cv.INTER_CUBIC)\n",
    "\n",
    "    imagesTest[i] = image\n",
    "    \n",
    "#normalises image data and one hot encodes the labels\n",
    "labelsTrain = labelsTrain['eye_color']\n",
    "labelsTest = labelsTest['eye_color']\n",
    "\n",
    "imagesTrain = normalize(imagesTrain,axis=1)\n",
    "imagesTest = normalize(imagesTest,axis=1)\n",
    "\n",
    "labelsTrain = to_categorical(labelsTrain)\n",
    "labelsTest = to_categorical(labelsTest)\n",
    "\n",
    "#creates generator functions\n",
    "datagen_train = ImageDataGenerator()\n",
    "#needed to save memory\n",
    "\n",
    "datagen_Test = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b3aa2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1512f292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53604e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN architecture definition\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(150,150,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add( Dense(128,activation='relu') )\n",
    "model.add( Dense(64,activation='relu') )\n",
    "model.add( Dense(5,activation='softmax') )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45c113e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 36992)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               4735104   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,836,933\n",
      "Trainable params: 4,836,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#prints model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e601794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets CNN model parameters\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3d8f76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\X99S5\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 32s 183ms/step - loss: 1.3226 - accuracy: 0.4136 - val_loss: 0.9979 - val_accuracy: 0.6052\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.7635 - accuracy: 0.6690 - val_loss: 0.6985 - val_accuracy: 0.6848\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.4784 - accuracy: 0.8031 - val_loss: 0.4459 - val_accuracy: 0.8016\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.3798 - accuracy: 0.8283 - val_loss: 0.3539 - val_accuracy: 0.8344\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.3521 - accuracy: 0.8428 - val_loss: 0.3603 - val_accuracy: 0.8368\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.3406 - accuracy: 0.8461 - val_loss: 0.3549 - val_accuracy: 0.8372\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.3196 - accuracy: 0.8578 - val_loss: 0.3402 - val_accuracy: 0.8380\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.3071 - accuracy: 0.8657 - val_loss: 0.3539 - val_accuracy: 0.8440\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.2849 - accuracy: 0.8734 - val_loss: 0.3605 - val_accuracy: 0.8420\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.2806 - accuracy: 0.8768 - val_loss: 0.3493 - val_accuracy: 0.8460\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.2678 - accuracy: 0.8824 - val_loss: 0.3506 - val_accuracy: 0.8476\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.2687 - accuracy: 0.8850 - val_loss: 0.3929 - val_accuracy: 0.8412\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.2442 - accuracy: 0.8949 - val_loss: 0.4249 - val_accuracy: 0.8440\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.2328 - accuracy: 0.9046 - val_loss: 0.4367 - val_accuracy: 0.8472\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 0.2226 - accuracy: 0.9088 - val_loss: 0.5194 - val_accuracy: 0.8396\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.2078 - accuracy: 0.9121 - val_loss: 0.5173 - val_accuracy: 0.8396\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.1926 - accuracy: 0.9219 - val_loss: 0.5035 - val_accuracy: 0.8432\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.1840 - accuracy: 0.9293 - val_loss: 0.5540 - val_accuracy: 0.8428\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 0.1602 - accuracy: 0.9398 - val_loss: 0.5791 - val_accuracy: 0.8460\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.1502 - accuracy: 0.9441 - val_loss: 0.6939 - val_accuracy: 0.8440\n"
     ]
    }
   ],
   "source": [
    "#Start CNN fit function\n",
    "history = model.fit( \n",
    "    datagen_train.flow(imagesTrain, labelsTrain, batch_size=100),\n",
    "    epochs=20,\n",
    "    validation_data=datagen_Test.flow(imagesTest, labelsTest, batch_size=100),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95048507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
